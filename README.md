
# ğŸ§  Gerador de Tarefas com LLM â€” GPT-2 Ajustado por InstruÃ§Ãµes

Modelo de linguagem **GPT-2 ajustado (fine-tuning)** para transformar comandos em linguagem natural em **tarefas estruturadas**, disponibilizado por meio de uma **API REST em Flask**.

Este projeto demonstra um pipeline completo de **engenharia de modelos de linguagem**:  
**curadoria de dados â†’ fine-tuning â†’ inferÃªncia â†’ disponibilizaÃ§Ã£o via API**.

---

## ğŸ¯ Problema

Comandos em linguagem natural sÃ£o ambÃ­guos e nÃ£o estruturados, o que dificulta sua integraÃ§Ã£o direta com sistemas de produtividade, agendas ou assistentes virtuais.

Exemplo:
> â€œme lembre de ligar para o mÃ©dicoâ€

Como transformar isso em algo que um sistema consiga executar?

---

## ğŸ’¡ SoluÃ§Ã£o

Este projeto ajusta o modelo **GPT-2** utilizando **dados no formato de instruÃ§Ãµes**, permitindo que o modelo:

- Entenda a intenÃ§Ã£o do usuÃ¡rio  
- Gere uma resposta clara  
- Retorne uma **estrutura padronizada de tarefa**, contendo:
  - Nome da tarefa
  - HorÃ¡rio sugerido  

O modelo treinado Ã© servido localmente por meio de uma **API Flask**, pronta para integraÃ§Ã£o com outros sistemas.

---

## ğŸ—ï¸ Arquitetura do Sistema

```

Prompt do UsuÃ¡rio
â†“
TokenizaÃ§Ã£o
â†“
GPT-2 Ajustado por InstruÃ§Ãµes
â†“
SaÃ­da Estruturada (TASK + TIME)
â†“
API REST (Flask)

````

---

## âš™ï¸ Detalhes TÃ©cnicos

- **Modelo base:** GPT-2  
- **TÃ©cnica:** Fine-tuning com instruÃ§Ãµes (instruction tuning)  
- **Frameworks:** PyTorch, Hugging Face Transformers  
- **ServiÃ§o:** Flask (API local)  
- **InferÃªncia:** Pipeline local  

---

## ğŸ§ª Exemplo de Uso

### RequisiÃ§Ã£o para a API
```bash
curl -X POST http://localhost:5000/generate \
     -H "Content-Type: application/json" \
     -d '{"prompt": "Crie uma tarefa para ir ao dentista"}'
````

### Resposta Esperada

```json
{
  "response": "Tarefa de consulta no dentista adicionada.\n[TASK: Consulta no dentista | TIME: 10:00]"
}
```

---

## ğŸ“‚ Conjunto de Dados (Formato de InstruÃ§Ã£o)

O modelo Ã© treinado com exemplos no seguinte formato:

```json
{
  "text": "UsuÃ¡rio: me lembre de regar as plantas\nAssistente:\nLembrete criado para regar as plantas.\n[TASK: Regar plantas | TIME: 09:00]"
}
```

Esse padrÃ£o ensina o modelo a:

* Reconhecer intenÃ§Ãµes
* Manter consistÃªncia na saÃ­da
* Sugerir horÃ¡rios coerentes com o contexto da tarefa

---

## ğŸš€ Funcionamento Interno

1. Curadoria e tokenizaÃ§Ã£o de um dataset customizado
2. Ajuste fino do GPT-2 com o `Trainer` da Hugging Face
3. ExportaÃ§Ã£o do modelo treinado
4. DisponibilizaÃ§Ã£o via API REST local

---

## ğŸ“ˆ Resultados

* Boa compreensÃ£o da intenÃ§Ã£o do usuÃ¡rio
* SaÃ­das estruturadas consistentes
* SugestÃ£o de horÃ¡rios realistas
* InferÃªncia local estÃ¡vel

---

## ğŸ§  Por Que Este Projeto Ã‰ Relevante?

Este repositÃ³rio vai alÃ©m do uso bÃ¡sico de LLMs e demonstra:

* CriaÃ§Ã£o de datasets orientados a tarefas reais
* Ajuste fino de modelos de linguagem
* TransformaÃ§Ã£o de texto livre em dados estruturados
* Deploy local pronto para integraÃ§Ã£o

AplicÃ¡vel a:

* Ferramentas de produtividade
* Agendas inteligentes
* Assistentes virtuais
* Backends conversacionais

---

## ğŸ” Exemplo de InferÃªncia Local

```python
from transformers import pipeline

generator = pipeline(
    "text-generation",
    model="./gpt2-tasker",
    tokenizer="./gpt2-tasker"
)

prompt = "UsuÃ¡rio: me lembre de ligar para o mÃ©dico\nAssistente:\n"
output = generator(prompt, max_length=100, do_sample=True)

print(output[0]["generated_text"])
```

---

## ğŸ‘¤ Sobre Mim

Este projeto faz parte da minha exploraÃ§Ã£o prÃ¡tica em:

* Modelos de Linguagem de Grande Escala (LLMs)
* Instruction tuning
* Curadoria de dados para aplicaÃ§Ãµes reais
* Engenharia de IA com foco em produto

Se vocÃª busca alguÃ©m que una **engenharia de modelos** com **visÃ£o prÃ¡tica de sistemas**, este projeto reflete exatamente isso.
